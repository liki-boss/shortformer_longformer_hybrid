{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7093697,
          "sourceType": "datasetVersion",
          "datasetId": 4088150
        },
        {
          "sourceId": 7123008,
          "sourceType": "datasetVersion",
          "datasetId": 4108736
        },
        {
          "sourceId": 7142534,
          "sourceType": "datasetVersion",
          "datasetId": 4122591
        }
      ],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### A Hybrid Approach to deriving context in long documents"
      ],
      "metadata": {
        "id": "at2GE34Down2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The introduction of the longformer architecture through the paper \"Longformer: The Long-document Transformer\" (and references in the subsequent paper on the BigBird architecture) bought together new approaches to handle documents in a resource efficient manner. The attention mechanism computed up untill then (RoBERTa model) was computationally intensive (O($n^2$)).\n",
        "\n",
        "The paper in question introduced us to the  sliding window attention mechanism, among others, which I felt was quite interesting as the optimality now decreased to a time complexity that is linear. It did so by employing fixed-size windows around each tokens, enabling large receptive fields with linear scalability(O(n x w)), thus offering a balance between local context importance and computational efficiency.\n",
        "\n",
        "The Longformer's approach emphasizes efficiency and captures information in large inputs significantly through long-text document analysis. It also introduces a sparse attention mechanism, where tokens attend to a fixed number of global tokens, reducing the computational complexity from quadratic to linear concerning the sequence length. A dilated sliding window is employed in multi-headed attention with varying dilation configurations per head, enabling increased receptive fields without substantial computational overhead, and enhancing performance in capturing both local and long-range contextual information.\n",
        "\n",
        "As Longformer increases the capacity to capture long-range dependencies, there is a potential risk of overfitting, especially when dealing with smaller datasets. The final longformer model, as available on huggingface, employs a combination of the global attention mechanism and sliding window attention mechanism. Training models with global attention mechanisms can be more challenging than models with local attention. This apporach of local attnetion and relative positional encoding (efficiently using memory) was pitched through the paper \"Shortformer: Better Language Modeling using Shorter Inputs\". Internally the long documents can be split into multiple shorter groups and then trained on the shortformer model. Although this doesn't theoretically perform better than the longformer model on long documents, it is computationally more efficient than the latter. Hence, I intend to build a grouping feature on top of the shortformer model and compare it with the performance of a longformer-base model(pre-trained)."
      ],
      "metadata": {
        "id": "IJen3y9Oown5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install autocorrect -qq\n",
        "from autocorrect import Speller"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:10:17.506970Z",
          "iopub.execute_input": "2023-12-04T17:10:17.507245Z",
          "iopub.status.idle": "2023-12-04T17:10:32.533965Z",
          "shell.execute_reply.started": "2023-12-04T17:10:17.507219Z",
          "shell.execute_reply": "2023-12-04T17:10:32.532798Z"
        },
        "trusted": true,
        "id": "PDgDjfzAown6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import ast\n",
        "import time\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-12-04T17:10:32.535918Z",
          "iopub.execute_input": "2023-12-04T17:10:32.536217Z",
          "iopub.status.idle": "2023-12-04T17:10:38.727988Z",
          "shell.execute_reply.started": "2023-12-04T17:10:32.536187Z",
          "shell.execute_reply": "2023-12-04T17:10:38.726840Z"
        },
        "trusted": true,
        "id": "r-RF6YdHown8",
        "outputId": "f70b240c-1e0f-4ab2-c48d-ab7e56c23036"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing how the autocorrect library functions. I intend to use this as the dataset consists of essays composed by students varying from 6th grade to the 12th and through visual glancing, I observed quite a unintentional spelling mistakes."
      ],
      "metadata": {
        "id": "KAihW-kPown9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "misspell = [\"conclution worksing\", \"consulating coclusion\"]\n",
        "spell = Speller(lang = 'en')\n",
        "for misspelled_items in misspell:\n",
        "    print(\"Before: \", misspelled_items, \"\\t After: \", spell(misspelled_items))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:10:38.729365Z",
          "iopub.execute_input": "2023-12-04T17:10:38.729918Z",
          "iopub.status.idle": "2023-12-04T17:10:38.822518Z",
          "shell.execute_reply.started": "2023-12-04T17:10:38.729874Z",
          "shell.execute_reply": "2023-12-04T17:10:38.820696Z"
        },
        "trusted": true,
        "id": "Ky7Ouom4own-",
        "outputId": "b923ce21-681a-4db4-8fed-9924fbd6379b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Before:  conclution worksing \t After:  conclusion working\nBefore:  consulating coclusion \t After:  consulting conclusion\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I intend to build the shortformer model on top of the RoBERTa model with the following specifications."
      ],
      "metadata": {
        "id": "dxr-RbFuown-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'roberta-large'\n",
        "MODEL_PATH = 'model'\n",
        "RUN_NAME = f\"{MODEL_NAME}-4\"\n",
        "\n",
        "MAX_LENGTH = 512\n",
        "DOC_STRIDE = 128 #Strides for skipping possible overlapping tokens\n",
        "\n",
        "config = {'train_batch_size': 4,\n",
        "          'valid_batch_size': 2,\n",
        "          'epochs': 5,\n",
        "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
        "          'max_grad_norm': 10,\n",
        "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "          'model_name': MODEL_NAME,\n",
        "          'max_length': MAX_LENGTH,\n",
        "          'doc_stride': DOC_STRIDE,\n",
        "          }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:10:38.825134Z",
          "iopub.execute_input": "2023-12-04T17:10:38.825522Z",
          "iopub.status.idle": "2023-12-04T17:10:38.853428Z",
          "shell.execute_reply.started": "2023-12-04T17:10:38.825487Z",
          "shell.execute_reply": "2023-12-04T17:10:38.852546Z"
        },
        "trusted": true,
        "id": "HsaHiPqsown_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 3rd party interface to visualize the model training and validation loss functions for each of the categories."
      ],
      "metadata": {
        "id": "Y5vkBlyYown_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    api_key = 'e47e017c812018f74ccafcc437b2d02fc71db05e'\n",
        "    wandb.login(key=api_key)\n",
        "    wandb.init(project=\"experimenting\", name=RUN_NAME, config=config)\n",
        "except:\n",
        "    print('Error in initializing Weights and Bias API')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:10:38.854562Z",
          "iopub.execute_input": "2023-12-04T17:10:38.855292Z",
          "iopub.status.idle": "2023-12-04T17:11:12.238256Z",
          "shell.execute_reply.started": "2023-12-04T17:10:38.855259Z",
          "shell.execute_reply": "2023-12-04T17:11:12.237392Z"
        },
        "trusted": true,
        "id": "CNfW4mzGowoA",
        "outputId": "0854b2f7-2fd7-4299-8260-acfeb6394f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlikithvp21\u001b[0m (\u001b[33mexperimenting\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20231204_171041-1shvkezr</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/experimenting/experimenting/runs/1shvkezr' target=\"_blank\">roberta-large-4</a></strong> to <a href='https://wandb.ai/experimenting/experimenting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/experimenting/experimenting' target=\"_blank\">https://wandb.ai/experimenting/experimenting</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/experimenting/experimenting/runs/1shvkezr' target=\"_blank\">https://wandb.ai/experimenting/experimenting/runs/1shvkezr</a>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_initial = pd.read_csv('/kaggle/input/project/train.csv')\n",
        "print(df_initial.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:11:12.239699Z",
          "iopub.execute_input": "2023-12-04T17:11:12.240787Z",
          "iopub.status.idle": "2023-12-04T17:11:13.997077Z",
          "shell.execute_reply.started": "2023-12-04T17:11:12.240750Z",
          "shell.execute_reply": "2023-12-04T17:11:13.996048Z"
        },
        "trusted": true,
        "id": "FVoFhZuXowoA",
        "outputId": "b61238c7-c51c-4ce9-b021-244e8f6ad573"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(144293, 8)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_initial.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:11:13.998420Z",
          "iopub.execute_input": "2023-12-04T17:11:13.999052Z",
          "iopub.status.idle": "2023-12-04T17:11:14.027307Z",
          "shell.execute_reply.started": "2023-12-04T17:11:13.999013Z",
          "shell.execute_reply": "2023-12-04T17:11:14.026313Z"
        },
        "trusted": true,
        "id": "PGhET6SXowoB",
        "outputId": "48964f2e-6f96-41aa-f83a-a662378c1bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n1  423A1CA112E2  1.622628e+12            230.0          312.0   \n2  423A1CA112E2  1.622628e+12            313.0          401.0   \n3  423A1CA112E2  1.622628e+12            402.0          758.0   \n4  423A1CA112E2  1.622628e+12            759.0          886.0   \n5  423A1CA112E2  1.622628e+12            887.0         1150.0   \n6  423A1CA112E2  1.622628e+12           1151.0         1533.0   \n7  423A1CA112E2  1.622628e+12           1534.0         1602.0   \n8  423A1CA112E2  1.622628e+12           1603.0         1890.0   \n9  423A1CA112E2  1.622628e+12           1891.0         2027.0   \n\n                                      discourse_text        discourse_type  \\\n0  Modern humans today are always on their phone....                  Lead   \n1  They are some really bad consequences when stu...              Position   \n2  Some certain areas in the United States ban ph...              Evidence   \n3  When people have phones, they know about certa...              Evidence   \n4  Driving is one of the way how to get around. P...                 Claim   \n5  That's why there's a thing that's called no te...              Evidence   \n6  Sometimes on the news there is either an accid...              Evidence   \n7  Phones are fine to use and it's also the best ...                 Claim   \n8  If you go through a problem and you can't find...              Evidence   \n9  The news always updated when people do somethi...  Concluding Statement   \n\n       discourse_type_num                                   predictionstring  \n0                  Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n1              Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n2              Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n3              Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n4                 Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  \n5              Evidence 3  163 164 165 166 167 168 169 170 171 172 173 17...  \n6              Evidence 4  211 212 213 214 215 216 217 218 219 220 221 22...  \n7                 Claim 2  282 283 284 285 286 287 288 289 290 291 292 29...  \n8              Evidence 5  297 298 299 300 301 302 303 304 305 306 307 30...  \n9  Concluding Statement 1  355 356 357 358 359 360 361 362 363 364 365 36...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>230.0</td>\n      <td>312.0</td>\n      <td>They are some really bad consequences when stu...</td>\n      <td>Position</td>\n      <td>Position 1</td>\n      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>313.0</td>\n      <td>401.0</td>\n      <td>Some certain areas in the United States ban ph...</td>\n      <td>Evidence</td>\n      <td>Evidence 1</td>\n      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>402.0</td>\n      <td>758.0</td>\n      <td>When people have phones, they know about certa...</td>\n      <td>Evidence</td>\n      <td>Evidence 2</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>759.0</td>\n      <td>886.0</td>\n      <td>Driving is one of the way how to get around. P...</td>\n      <td>Claim</td>\n      <td>Claim 1</td>\n      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>887.0</td>\n      <td>1150.0</td>\n      <td>That's why there's a thing that's called no te...</td>\n      <td>Evidence</td>\n      <td>Evidence 3</td>\n      <td>163 164 165 166 167 168 169 170 171 172 173 17...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>1151.0</td>\n      <td>1533.0</td>\n      <td>Sometimes on the news there is either an accid...</td>\n      <td>Evidence</td>\n      <td>Evidence 4</td>\n      <td>211 212 213 214 215 216 217 218 219 220 221 22...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>1534.0</td>\n      <td>1602.0</td>\n      <td>Phones are fine to use and it's also the best ...</td>\n      <td>Claim</td>\n      <td>Claim 2</td>\n      <td>282 283 284 285 286 287 288 289 290 291 292 29...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>1603.0</td>\n      <td>1890.0</td>\n      <td>If you go through a problem and you can't find...</td>\n      <td>Evidence</td>\n      <td>Evidence 5</td>\n      <td>297 298 299 300 301 302 303 304 305 306 307 30...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>1891.0</td>\n      <td>2027.0</td>\n      <td>The news always updated when people do somethi...</td>\n      <td>Concluding Statement</td>\n      <td>Concluding Statement 1</td>\n      <td>355 356 357 358 359 360 361 362 363 364 365 36...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_initial['predictionstring'][1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:11:14.028586Z",
          "iopub.execute_input": "2023-12-04T17:11:14.029145Z",
          "iopub.status.idle": "2023-12-04T17:11:14.036450Z",
          "shell.execute_reply.started": "2023-12-04T17:11:14.029108Z",
          "shell.execute_reply": "2023-12-04T17:11:14.035418Z"
        },
        "trusted": true,
        "id": "PAa8al1zowoB",
        "outputId": "059ac2a0-4e5e-4ca3-b5e7-8413adc3c32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'45 46 47 48 49 50 51 52 53 54 55 56 57 58 59'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Named Entity Recognition:\n",
        "\n",
        "We shall now convert the data to NER labels and save them."
      ],
      "metadata": {
        "id": "0HZOCg1oowoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/train-ner/train_NER.csv\"):\n",
        "    df_texts = pd.read_csv(\"/kaggle/input/train-ner/train_NER.csv\", converters={'entities':ast.literal_eval, 'text_split': ast.literal_eval})\n",
        "else:\n",
        "    train_names, train_texts = [], []\n",
        "    for f in tqdm(list(os.listdir('../input/project/train'))):\n",
        "        train_names.append(f.replace('.txt', ''))\n",
        "        train_texts.append(spell(open('../input/project/train/' + f, 'r').read()))\n",
        "\n",
        "        df_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\n",
        "    df_texts['text_split'] = df_texts.text.str.split()\n",
        "\n",
        "    all_entities = []\n",
        "    for _, row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n",
        "        total = len(row['text_split'])\n",
        "        entities = [\"O\"] * total\n",
        "        for _, row2 in df_initial[df_initial['id'] == row['id']].iterrows():\n",
        "            discourse = row2['discourse_type']\n",
        "            list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n",
        "            entities[list_ix[0]] = f\"B-{discourse}\"\n",
        "            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n",
        "        all_entities.append(entities)\n",
        "    df_texts['entities'] = all_entities\n",
        "    df_texts.to_csv('../input/project/train_NER.csv',index=False)\n",
        "\n",
        "print(df_texts.shape)\n",
        "df_texts.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:18:24.641045Z",
          "iopub.execute_input": "2023-12-04T17:18:24.641833Z",
          "iopub.status.idle": "2023-12-04T17:18:54.896510Z",
          "shell.execute_reply.started": "2023-12-04T17:18:24.641797Z",
          "shell.execute_reply": "2023-12-04T17:18:54.895397Z"
        },
        "trusted": true,
        "id": "7BvvLoITowoC",
        "outputId": "53133eba-5573-46b4-c8f4-4052e8eee67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(15594, 4)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "             id                                               text  \\\n0  62C57C524CD2  I think we should be able to play in a sport i...   \n1  80667AD3FFD8  Some schools require summer projects for stude...   \n2  21868C40B94F  Driverless cars have been argued and talked ab...   \n3  87A6EF3113C6  The author of \"The Challenge of Exploring Venu...   \n4  24687D08CFDA  Wow, from the mar really look like humans face...   \n\n                                          text_split  \\\n0  [I, think, we, should, be, able, to, play, in,...   \n1  [Some, schools, require, summer, projects, for...   \n2  [Driverless, cars, have, been, argued, and, ta...   \n3  [The, author, of, \"The, Challenge, of, Explori...   \n4  [Wow,, from, the, mar, really, look, like, hum...   \n\n                                            entities  \n0  [B-Position, I-Position, I-Position, I-Positio...  \n1  [B-Position, I-Position, I-Position, I-Positio...  \n2  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n3  [B-Position, I-Position, I-Position, I-Positio...  \n4  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_split</th>\n      <th>entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>62C57C524CD2</td>\n      <td>I think we should be able to play in a sport i...</td>\n      <td>[I, think, we, should, be, able, to, play, in,...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80667AD3FFD8</td>\n      <td>Some schools require summer projects for stude...</td>\n      <td>[Some, schools, require, summer, projects, for...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21868C40B94F</td>\n      <td>Driverless cars have been argued and talked ab...</td>\n      <td>[Driverless, cars, have, been, argued, and, ta...</td>\n      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>87A6EF3113C6</td>\n      <td>The author of \"The Challenge of Exploring Venu...</td>\n      <td>[The, author, of, \"The, Challenge, of, Explori...</td>\n      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24687D08CFDA</td>\n      <td>Wow, from the mar really look like humans face...</td>\n      <td>[Wow,, from, the, mar, really, look, like, hum...</td>\n      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(df_texts['text_split'].str.len() == df_texts['entities'].str.len()).all()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:19:03.944304Z",
          "iopub.execute_input": "2023-12-04T17:19:03.944698Z",
          "iopub.status.idle": "2023-12-04T17:19:03.980947Z",
          "shell.execute_reply.started": "2023-12-04T17:19:03.944665Z",
          "shell.execute_reply": "2023-12-04T17:19:03.979805Z"
        },
        "trusted": true,
        "id": "lbmOg86GowoD",
        "outputId": "5306886d-5bac-4f7c-cdae-9b8cc3e9773a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n",
        "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
        "\n",
        "LABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\n",
        "IDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\n",
        "\n",
        "LABELS_TO_IDS"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:19:04.297115Z",
          "iopub.execute_input": "2023-12-04T17:19:04.297516Z",
          "iopub.status.idle": "2023-12-04T17:19:04.307754Z",
          "shell.execute_reply.started": "2023-12-04T17:19:04.297482Z",
          "shell.execute_reply": "2023-12-04T17:19:04.306650Z"
        },
        "trusted": true,
        "id": "qqdnwuWdowoD",
        "outputId": "20cf5d0b-e744-4a37-cb1d-5d8710a43731"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'O': 0,\n 'B-Lead': 1,\n 'I-Lead': 2,\n 'B-Position': 3,\n 'I-Position': 4,\n 'B-Claim': 5,\n 'I-Claim': 6,\n 'B-Counterclaim': 7,\n 'I-Counterclaim': 8,\n 'B-Rebuttal': 9,\n 'I-Rebuttal': 10,\n 'B-Evidence': 11,\n 'I-Evidence': 12,\n 'B-Concluding Statement': 13,\n 'I-Concluding Statement': 14}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IDS = df_initial.id.unique()\n",
        "print(f'There are {len(IDS)} train texts. We will split 90% 10% for validation.')\n",
        "np.random.seed(42)\n",
        "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
        "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
        "np.random.seed(None)\n",
        "\n",
        "df_train = df_texts.loc[df_texts['id'].isin(IDS[train_idx])].reset_index(drop=True)\n",
        "df_val = df_texts.loc[df_texts['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
        "\n",
        "print(f\"FULL Dataset : {df_texts.shape}\")\n",
        "print(f\"TRAIN Dataset: {df_train.shape}\")\n",
        "print(f\"TEST Dataset : {df_val.shape}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:19:06.917992Z",
          "iopub.execute_input": "2023-12-04T17:19:06.918383Z",
          "iopub.status.idle": "2023-12-04T17:19:06.969394Z",
          "shell.execute_reply.started": "2023-12-04T17:19:06.918326Z",
          "shell.execute_reply": "2023-12-04T17:19:06.967722Z"
        },
        "trusted": true,
        "id": "Dek90QT_owoE",
        "outputId": "cb22cb50-7de3-4fe7-e9bc-a7f168007dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 15594 train texts. We will split 90% 10% for validation.\nFULL Dataset : (15594, 4)\nTRAIN Dataset: (14034, 4)\nTEST Dataset : (1560, 4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the RoBERTa Model with the configurations."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T20:00:31.509044Z",
          "iopub.execute_input": "2023-12-10T20:00:31.509954Z",
          "iopub.status.idle": "2023-12-10T20:00:31.520965Z",
          "shell.execute_reply.started": "2023-12-10T20:00:31.509915Z",
          "shell.execute_reply": "2023-12-10T20:00:31.519794Z"
        },
        "id": "Ey-9m-deowoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_model():\n",
        "    os.mkdir(MODEL_PATH)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
        "    tokenizer.save_pretrained(MODEL_PATH)\n",
        "    config_model = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    config_model.num_labels = 15\n",
        "    config_model.save_pretrained(MODEL_PATH)\n",
        "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config_model)\n",
        "    backbone.save_pretrained(os.path.join(MODEL_PATH, 'pytorch_model.bin'))\n",
        "    print(f\"Model downloaded to {MODEL_PATH}/\")\n",
        "download_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:20:26.115295Z",
          "iopub.execute_input": "2023-12-04T17:20:26.116386Z",
          "iopub.status.idle": "2023-12-04T17:20:32.356172Z",
          "shell.execute_reply.started": "2023-12-04T17:20:26.116320Z",
          "shell.execute_reply": "2023-12-04T17:20:32.352826Z"
        },
        "trusted": true,
        "id": "GPSErD7HowoE",
        "outputId": "1ccf86dd-fb32-42e4-acac-2798aafc079d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model downloaded to model/\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /kaggle/working/model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:19:20.341577Z",
          "iopub.execute_input": "2023-12-04T17:19:20.342839Z",
          "iopub.status.idle": "2023-12-04T17:19:21.555812Z",
          "shell.execute_reply.started": "2023-12-04T17:19:20.342790Z",
          "shell.execute_reply": "2023-12-04T17:19:21.554330Z"
        },
        "trusted": true,
        "id": "7sMU6z1LowoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:20:35.913816Z",
          "iopub.execute_input": "2023-12-04T17:20:35.914208Z",
          "iopub.status.idle": "2023-12-04T17:20:36.060514Z",
          "shell.execute_reply.started": "2023-12-04T17:20:35.914174Z",
          "shell.execute_reply": "2023-12-04T17:20:36.059410Z"
        },
        "trusted": true,
        "id": "iBs1CGmLowoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(word_ids, word_labels):\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "        else:\n",
        "            label_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n",
        "    return label_ids"
      ],
      "metadata": {
        "id": "jMQqO6EsowoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(df, to_tensor=True, with_labels=True):\n",
        "    encoded = tokenizer(df['text_split'].tolist(),\n",
        "                        is_split_into_words=True,\n",
        "                        return_overflowing_tokens=True,\n",
        "                        stride=DOC_STRIDE,\n",
        "                        max_length=MAX_LENGTH,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True)\n",
        "\n",
        "    if with_labels:\n",
        "        encoded['labels'] = []\n",
        "    encoded['wids'] = []\n",
        "    n = len(encoded['overflow_to_sample_mapping'])\n",
        "    for i in range(n):\n",
        "        text_idx = encoded['overflow_to_sample_mapping'][i]\n",
        "        word_ids = encoded.word_ids(i)\n",
        "\n",
        "        if with_labels:\n",
        "            word_labels = df['entities'].iloc[text_idx]\n",
        "            label_ids = get_labels(word_ids, word_labels)\n",
        "            encoded['labels'].append(label_ids)\n",
        "        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n",
        "\n",
        "    if to_tensor:\n",
        "        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n",
        "    return encoded"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:20:37.017670Z",
          "iopub.execute_input": "2023-12-04T17:20:37.018540Z",
          "iopub.status.idle": "2023-12-04T17:20:37.031318Z",
          "shell.execute_reply.started": "2023-12-04T17:20:37.018503Z",
          "shell.execute_reply": "2023-12-04T17:20:37.030294Z"
        },
        "trusted": true,
        "id": "x8xMedK2owoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = tokenize(df_train)\n",
        "tokenized_val = tokenize(df_val)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:20:37.354562Z",
          "iopub.execute_input": "2023-12-04T17:20:37.355160Z",
          "iopub.status.idle": "2023-12-04T17:21:19.934829Z",
          "shell.execute_reply.started": "2023-12-04T17:20:37.355128Z",
          "shell.execute_reply": "2023-12-04T17:21:19.933662Z"
        },
        "trusted": true,
        "id": "DEp25880owoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train['overflow_to_sample_mapping'][:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:19.937099Z",
          "iopub.execute_input": "2023-12-04T17:21:19.937529Z",
          "iopub.status.idle": "2023-12-04T17:21:19.952790Z",
          "shell.execute_reply.started": "2023-12-04T17:21:19.937491Z",
          "shell.execute_reply": "2023-12-04T17:21:19.951868Z"
        },
        "trusted": true,
        "id": "HqckJW4XowoG",
        "outputId": "5bb4b5ac-a515-4083-c456-f24c5cb55f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([0, 1, 1, 2, 2, 3, 4, 5, 5, 6])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedbackPrizeDataset(Dataset):\n",
        "    def __init__(self, tokenized_ds):\n",
        "        self.data = tokenized_ds\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = {k: self.data[k][index] for k in self.data.keys()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])"
      ],
      "metadata": {
        "id": "Hw4QOFuBowoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = FeedbackPrizeDataset(tokenized_train)\n",
        "dl_train = DataLoader(ds_train, batch_size=config['train_batch_size'],\n",
        "                      shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "ds_val = FeedbackPrizeDataset(tokenized_val)\n",
        "dl_val = DataLoader(ds_val, batch_size=config['valid_batch_size'],\n",
        "                    shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:19.954021Z",
          "iopub.execute_input": "2023-12-04T17:21:19.954401Z",
          "iopub.status.idle": "2023-12-04T17:21:19.966910Z",
          "shell.execute_reply.started": "2023-12-04T17:21:19.954364Z",
          "shell.execute_reply": "2023-12-04T17:21:19.965978Z"
        },
        "trusted": true,
        "id": "JeR79iTGowoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will employ a gradient clipping method to prevent exploding of the gradients and log the training metrics through a third-party API Weights & Biases for visualization."
      ],
      "metadata": {
        "id": "_h6Yak8mowoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, dl_train, epoch):\n",
        "    time_start = time.time()\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = config['learning_rates'][epoch]\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    epoch_prefix = f\"[Epoch {epoch+1:2d} / {config['epochs']:2d}]\"\n",
        "    print(f\"{epoch_prefix} Starting epoch {epoch+1:2d} with LR = {lr}\")\n",
        "\n",
        "    model.train()\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for idx, batch in enumerate(dl_train):\n",
        "        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n",
        "        labels = batch['labels'].to(config['device'], dtype = torch.long)\n",
        "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n",
        "                               return_dict=False)\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        loss_step = tr_loss/nb_tr_steps\n",
        "        if idx % 200 == 0:\n",
        "            print(f\"{epoch_prefix}     Steps: {idx:4d} --> Loss: {loss_step:.4f}\")\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "        wandb.log({'Train Loss (Step)': loss_step, 'Train Accuracy (Step)' : tr_accuracy / nb_tr_steps})\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "\n",
        "    torch.save(model.state_dict(), f'pytorch_model_e{epoch}.bin')\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    elapsed = time.time() - time_start\n",
        "\n",
        "    print(epoch_prefix)\n",
        "    print(f\"{epoch_prefix} Training loss    : {epoch_loss:.4f}\")\n",
        "    print(f\"{epoch_prefix} Training accuracy: {tr_accuracy:.4f}\")\n",
        "    print(f\"{epoch_prefix} Model saved to pytorch_model_e{epoch}.bin  [{elapsed/60:.2f} mins]\")\n",
        "    wandb.log({'Train Loss (Epoch)': epoch_loss, 'Train Accuracy (Epoch)' : tr_accuracy})\n",
        "    print(epoch_prefix)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:19.969954Z",
          "iopub.execute_input": "2023-12-04T17:21:19.970332Z",
          "iopub.status.idle": "2023-12-04T17:21:19.986624Z",
          "shell.execute_reply.started": "2023-12-04T17:21:19.970298Z",
          "shell.execute_reply": "2023-12-04T17:21:19.985655Z"
        },
        "trusted": true,
        "id": "1mS70VeCowoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_overlap(row): #Calculate the overlap between prediction and truth\n",
        "    set_pred = set(row.predictionstring_pred.split(' '))\n",
        "    set_gt = set(row.predictionstring_gt.split(' '))\n",
        "    len_gt = len(set_gt)\n",
        "    len_pred = len(set_pred)\n",
        "    inter = len(set_gt.intersection(set_pred))\n",
        "    overlap_1 = inter / len_gt\n",
        "    overlap_2 = inter/ len_pred\n",
        "    return [overlap_1, overlap_2]"
      ],
      "metadata": {
        "id": "KjDbPUEXowoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation based on the overlap between ground truth and predicted word indices.\n",
        "\n",
        "1. For each sample, all ground truths and predictions for a given class are compared.\n",
        "2. If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
        "3. Any unmatched ground truths are false negatives and any unmatched predictions are false positives."
      ],
      "metadata": {
        "id": "1rQhj28jowoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_feedback_comp(pred_df, gt_df):\n",
        "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
        "        .reset_index(drop=True).copy()\n",
        "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
        "        .reset_index(drop=True).copy()\n",
        "    pred_df['pred_id'] = pred_df.index\n",
        "    gt_df['gt_id'] = gt_df.index\n",
        "    # Step 1. all ground truths and predictions for a given class are compared.\n",
        "    joined = pred_df.merge(gt_df,\n",
        "                           left_on=['id','class'],\n",
        "                           right_on=['id','discourse_type'],\n",
        "                           how='outer',\n",
        "                           suffixes=('_pred','_gt')\n",
        "                          )\n",
        "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
        "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
        "\n",
        "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
        "\n",
        "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
        "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
        "    # the prediction is a match and considered a true positive.\n",
        "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
        "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
        "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
        "\n",
        "\n",
        "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
        "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
        "    tp_pred_ids = joined.query('potential_TP') \\\n",
        "        .sort_values('max_overlap', ascending=False) \\\n",
        "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
        "\n",
        "    # 3. Any unmatched ground truths are false negatives\n",
        "    # and any unmatched predictions are false positives.\n",
        "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
        "\n",
        "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
        "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
        "\n",
        "    # Get numbers of each type\n",
        "    TP = len(tp_pred_ids)\n",
        "    FP = len(fp_pred_ids)\n",
        "    FN = len(unmatched_gt_ids)\n",
        "    #calc microf1\n",
        "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
        "    return my_f1_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:19.988283Z",
          "iopub.execute_input": "2023-12-04T17:21:19.988704Z",
          "iopub.status.idle": "2023-12-04T17:21:20.007449Z",
          "shell.execute_reply.started": "2023-12-04T17:21:19.988672Z",
          "shell.execute_reply": "2023-12-04T17:21:20.006266Z"
        },
        "trusted": true,
        "id": "W1eX7gQLowoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I dynamically aggregate the predictions across chunks by tracking word-level indices and maintaining accumulated data for each text ID, ensuring\n",
        "seamless merging of predictions for samples split across different batches."
      ],
      "metadata": {
        "id": "PRhTHIVLowoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(dl):\n",
        "    predictions = defaultdict(list) #For text-level data, to help in the merging process by data accumulation of all groups of data\n",
        "    seen_words_idx = defaultdict(list)\n",
        "\n",
        "    for batch in dl:\n",
        "        ids = batch[\"input_ids\"].to(config['device'])\n",
        "        mask = batch[\"attention_mask\"].to(config['device'])\n",
        "        outputs = model(ids, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        del ids, mask\n",
        "\n",
        "        batch_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy()\n",
        "\n",
        "        # Go over each prediction, getting the text_id reference\n",
        "        for k, (chunk_preds, text_id) in enumerate(zip(batch_preds, batch['overflow_to_sample_mapping'].tolist())):\n",
        "\n",
        "            # The word_ids are absolute references in the original text\n",
        "            word_ids = batch['wids'][k].numpy()\n",
        "\n",
        "            # Map from ids to labels\n",
        "            chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]\n",
        "\n",
        "            for idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx == -1:\n",
        "                    pass\n",
        "                elif word_idx not in seen_words_idx[text_id]:\n",
        "                    # Add predictions if the word doesn't have a prediction from a previous chunk\n",
        "                    predictions[text_id].append(chunk_preds[idx])\n",
        "                    seen_words_idx[text_id].append(word_idx)\n",
        "\n",
        "    final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n",
        "    return final_predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:20.008889Z",
          "iopub.execute_input": "2023-12-04T17:21:20.009235Z",
          "iopub.status.idle": "2023-12-04T17:21:20.026225Z",
          "shell.execute_reply.started": "2023-12-04T17:21:20.009204Z",
          "shell.execute_reply": "2023-12-04T17:21:20.025236Z"
        },
        "trusted": true,
        "id": "fiVC-ricowoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(df, dl):\n",
        "\n",
        "    all_labels = inference(dl)\n",
        "    final_preds = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        idx = df.id.values[i]\n",
        "        pred = all_labels[i]\n",
        "        preds = []\n",
        "        j = 0\n",
        "\n",
        "        while j < len(pred):\n",
        "            cls = pred[j]\n",
        "            if cls == 'O': pass\n",
        "            else: cls = cls.replace('B','I')\n",
        "            end = j + 1\n",
        "            while end < len(pred) and pred[end] == cls:\n",
        "                end += 1\n",
        "            if cls != 'O' and cls != '' and end - j > 7:\n",
        "                final_preds.append((idx, cls.replace('I-',''),\n",
        "                                    ' '.join(map(str, list(range(j, end))))))\n",
        "            j = end\n",
        "\n",
        "    df_pred = pd.DataFrame(final_preds)\n",
        "    df_pred.columns = ['id','class','predictionstring']\n",
        "    return df_pred"
      ],
      "metadata": {
        "id": "WNNiinulowoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, df_all, df_val, dl_val, epoch):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    # Put model in eval model\n",
        "    model.eval()\n",
        "\n",
        "    # Valid targets: needed because df_val has a subset of the columns\n",
        "    df_valid = df_all.loc[df_all['id'].isin(IDS[valid_idx])]\n",
        "\n",
        "    # OOF predictions\n",
        "    oof = get_predictions(df_val, dl_val)\n",
        "\n",
        "    # Compute F1-score\n",
        "    f1s = []\n",
        "    classes = oof['class'].unique()\n",
        "\n",
        "    epoch_prefix = f\"[Epoch {epoch+1:2d} / {config['epochs']:2d}]\"\n",
        "    print(f\"{epoch_prefix} Validation F1 scores\")\n",
        "\n",
        "    f1s_log = {}\n",
        "    for c in classes:\n",
        "        pred_df = oof.loc[oof['class']==c].copy()\n",
        "        gt_df = df_valid.loc[df_valid['discourse_type']==c].copy()\n",
        "        f1 = score_feedback_comp(pred_df, gt_df)\n",
        "        print(f\"{epoch_prefix}   * {c:<10}: {f1:4f}\")\n",
        "        f1s.append(f1)\n",
        "        f1s_log[f'F1 {c}'] = f1\n",
        "\n",
        "    elapsed = time.time() - time_start\n",
        "    print(epoch_prefix)\n",
        "    print(f'{epoch_prefix} Overall Validation F1: {np.mean(f1s):.4f} [{elapsed:.2f} secs]')\n",
        "    print(epoch_prefix)\n",
        "    f1s_log['Overall F1'] = np.mean(f1s)\n",
        "    wandb.log(f1s_log)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:20.027569Z",
          "iopub.execute_input": "2023-12-04T17:21:20.028227Z",
          "iopub.status.idle": "2023-12-04T17:21:20.041545Z",
          "shell.execute_reply.started": "2023-12-04T17:21:20.028194Z",
          "shell.execute_reply": "2023-12-04T17:21:20.040554Z"
        },
        "trusted": true,
        "id": "tQDpWXCtowoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_model = AutoConfig.from_pretrained('/kaggle/working/model/config.json')\n",
        "model = AutoModelForTokenClassification.from_pretrained('/kaggle/working/model/pytorch_model.bin',config=config_model)\n",
        "model.to(config['device']);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:20.042839Z",
          "iopub.execute_input": "2023-12-04T17:21:20.043192Z",
          "iopub.status.idle": "2023-12-04T17:21:26.644711Z",
          "shell.execute_reply.started": "2023-12-04T17:21:20.043167Z",
          "shell.execute_reply": "2023-12-04T17:21:26.643655Z"
        },
        "trusted": true,
        "id": "NcUHcpG2owoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])\n",
        "\n",
        "# Loop\n",
        "for epoch in range(config['epochs']):\n",
        "    print()\n",
        "    train(model, optimizer, dl_train, epoch)\n",
        "    validate(model, df_initial, df_val, dl_val, epoch)\n",
        "\n",
        "print(\"Final model saved as 'pytorch_model.bin'\")\n",
        "torch.save(model.state_dict(), 'pytorch_model.bin')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T17:21:26.646493Z",
          "iopub.execute_input": "2023-12-04T17:21:26.646867Z",
          "iopub.status.idle": "2023-12-04T22:58:41.857911Z",
          "shell.execute_reply.started": "2023-12-04T17:21:26.646831Z",
          "shell.execute_reply": "2023-12-04T22:58:41.856846Z"
        },
        "trusted": true,
        "id": "FtwRT-nNowoL",
        "outputId": "778dd99b-b516-4337-eb5e-84ed7a9d86d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n[Epoch  1 /  5] Starting epoch  1 with LR = 2.5e-05\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  1 /  5]     Steps:    0 --> Loss: 3.9966\n[Epoch  1 /  5]     Steps:  200 --> Loss: 1.2659\n[Epoch  1 /  5]     Steps:  400 --> Loss: 1.0732\n[Epoch  1 /  5]     Steps:  600 --> Loss: 0.9930\n[Epoch  1 /  5]     Steps:  800 --> Loss: 0.9400\n[Epoch  1 /  5]     Steps: 1000 --> Loss: 0.9127\n[Epoch  1 /  5]     Steps: 1200 --> Loss: 0.8890\n[Epoch  1 /  5]     Steps: 1400 --> Loss: 0.8706\n[Epoch  1 /  5]     Steps: 1600 --> Loss: 0.8537\n[Epoch  1 /  5]     Steps: 1800 --> Loss: 0.8343\n[Epoch  1 /  5]     Steps: 2000 --> Loss: 0.8235\n[Epoch  1 /  5]     Steps: 2200 --> Loss: 0.8131\n[Epoch  1 /  5]     Steps: 2400 --> Loss: 0.8015\n[Epoch  1 /  5]     Steps: 2600 --> Loss: 0.7933\n[Epoch  1 /  5]     Steps: 2800 --> Loss: 0.7857\n[Epoch  1 /  5]     Steps: 3000 --> Loss: 0.7865\n[Epoch  1 /  5]     Steps: 3200 --> Loss: 0.7808\n[Epoch  1 /  5]     Steps: 3400 --> Loss: 0.7765\n[Epoch  1 /  5]     Steps: 3600 --> Loss: 0.7731\n[Epoch  1 /  5]     Steps: 3800 --> Loss: 0.7674\n[Epoch  1 /  5]     Steps: 4000 --> Loss: 0.7640\n[Epoch  1 /  5]     Steps: 4200 --> Loss: 0.7589\n[Epoch  1 /  5]     Steps: 4400 --> Loss: 0.7553\n[Epoch  1 /  5]     Steps: 4600 --> Loss: 0.7504\n[Epoch  1 /  5]     Steps: 4800 --> Loss: 0.7483\n[Epoch  1 /  5]\n[Epoch  1 /  5] Training loss    : 0.7463\n[Epoch  1 /  5] Training accuracy: 0.7551\n[Epoch  1 /  5] Model saved to pytorch_model_e0.bin  [65.03 mins]\n[Epoch  1 /  5]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  1 /  5] Validation F1 scores\n[Epoch  1 /  5]   * Lead      : 0.772864\n[Epoch  1 /  5]   * Claim     : 0.511492\n[Epoch  1 /  5]   * Evidence  : 0.621083\n[Epoch  1 /  5]   * Counterclaim: 0.456814\n[Epoch  1 /  5]   * Rebuttal  : 0.359574\n[Epoch  1 /  5]   * Concluding Statement: 0.730453\n[Epoch  1 /  5]   * Position  : 0.660455\n[Epoch  1 /  5]\n[Epoch  1 /  5] Overall Validation F1: 0.5875 [147.82 secs]\n[Epoch  1 /  5]\n\n[Epoch  2 /  5] Starting epoch  2 with LR = 2.5e-05\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  2 /  5]     Steps:    0 --> Loss: 0.3913\n[Epoch  2 /  5]     Steps:  200 --> Loss: 0.6246\n[Epoch  2 /  5]     Steps:  400 --> Loss: 0.6129\n[Epoch  2 /  5]     Steps:  600 --> Loss: 0.6122\n[Epoch  2 /  5]     Steps:  800 --> Loss: 0.6138\n[Epoch  2 /  5]     Steps: 1000 --> Loss: 0.6221\n[Epoch  2 /  5]     Steps: 1200 --> Loss: 0.6192\n[Epoch  2 /  5]     Steps: 1400 --> Loss: 0.6174\n[Epoch  2 /  5]     Steps: 1600 --> Loss: 0.6196\n[Epoch  2 /  5]     Steps: 1800 --> Loss: 0.6175\n[Epoch  2 /  5]     Steps: 2000 --> Loss: 0.6142\n[Epoch  2 /  5]     Steps: 2200 --> Loss: 0.6109\n[Epoch  2 /  5]     Steps: 2400 --> Loss: 0.6094\n[Epoch  2 /  5]     Steps: 2600 --> Loss: 0.6075\n[Epoch  2 /  5]     Steps: 2800 --> Loss: 0.6055\n[Epoch  2 /  5]     Steps: 3000 --> Loss: 0.6048\n[Epoch  2 /  5]     Steps: 3200 --> Loss: 0.6062\n[Epoch  2 /  5]     Steps: 3400 --> Loss: 0.6146\n[Epoch  2 /  5]     Steps: 3600 --> Loss: 0.6137\n[Epoch  2 /  5]     Steps: 3800 --> Loss: 0.6130\n[Epoch  2 /  5]     Steps: 4000 --> Loss: 0.6131\n[Epoch  2 /  5]     Steps: 4200 --> Loss: 0.6124\n[Epoch  2 /  5]     Steps: 4400 --> Loss: 0.6105\n[Epoch  2 /  5]     Steps: 4600 --> Loss: 0.6098\n[Epoch  2 /  5]     Steps: 4800 --> Loss: 0.6110\n[Epoch  2 /  5]\n[Epoch  2 /  5] Training loss    : 0.6094\n[Epoch  2 /  5] Training accuracy: 0.7923\n[Epoch  2 /  5] Model saved to pytorch_model_e1.bin  [65.08 mins]\n[Epoch  2 /  5]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  2 /  5] Validation F1 scores\n[Epoch  2 /  5]   * Position  : 0.641034\n[Epoch  2 /  5]   * Lead      : 0.711233\n[Epoch  2 /  5]   * Claim     : 0.501585\n[Epoch  2 /  5]   * Evidence  : 0.626982\n[Epoch  2 /  5]   * Counterclaim: 0.511290\n[Epoch  2 /  5]   * Concluding Statement: 0.765943\n[Epoch  2 /  5]   * Rebuttal  : 0.410693\n[Epoch  2 /  5]\n[Epoch  2 /  5] Overall Validation F1: 0.5955 [147.85 secs]\n[Epoch  2 /  5]\n\n[Epoch  3 /  5] Starting epoch  3 with LR = 2.5e-06\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  3 /  5]     Steps:    0 --> Loss: 0.2295\n[Epoch  3 /  5]     Steps:  200 --> Loss: 0.5102\n[Epoch  3 /  5]     Steps:  400 --> Loss: 0.5046\n[Epoch  3 /  5]     Steps:  600 --> Loss: 0.5013\n[Epoch  3 /  5]     Steps:  800 --> Loss: 0.5030\n[Epoch  3 /  5]     Steps: 1000 --> Loss: 0.4983\n[Epoch  3 /  5]     Steps: 1200 --> Loss: 0.4917\n[Epoch  3 /  5]     Steps: 1400 --> Loss: 0.4888\n[Epoch  3 /  5]     Steps: 1600 --> Loss: 0.4864\n[Epoch  3 /  5]     Steps: 1800 --> Loss: 0.4840\n[Epoch  3 /  5]     Steps: 2000 --> Loss: 0.4795\n[Epoch  3 /  5]     Steps: 2200 --> Loss: 0.4774\n[Epoch  3 /  5]     Steps: 2400 --> Loss: 0.4766\n[Epoch  3 /  5]     Steps: 2600 --> Loss: 0.4766\n[Epoch  3 /  5]     Steps: 2800 --> Loss: 0.4739\n[Epoch  3 /  5]     Steps: 3000 --> Loss: 0.4714\n[Epoch  3 /  5]     Steps: 3200 --> Loss: 0.4697\n[Epoch  3 /  5]     Steps: 3400 --> Loss: 0.4683\n[Epoch  3 /  5]     Steps: 3600 --> Loss: 0.4678\n[Epoch  3 /  5]     Steps: 3800 --> Loss: 0.4669\n[Epoch  3 /  5]     Steps: 4000 --> Loss: 0.4663\n[Epoch  3 /  5]     Steps: 4200 --> Loss: 0.4670\n[Epoch  3 /  5]     Steps: 4400 --> Loss: 0.4671\n[Epoch  3 /  5]     Steps: 4600 --> Loss: 0.4663\n[Epoch  3 /  5]     Steps: 4800 --> Loss: 0.4668\n[Epoch  3 /  5]\n[Epoch  3 /  5] Training loss    : 0.4660\n[Epoch  3 /  5] Training accuracy: 0.8353\n[Epoch  3 /  5] Model saved to pytorch_model_e2.bin  [65.03 mins]\n[Epoch  3 /  5]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  3 /  5] Validation F1 scores\n[Epoch  3 /  5]   * Lead      : 0.787592\n[Epoch  3 /  5]   * Claim     : 0.540274\n[Epoch  3 /  5]   * Evidence  : 0.675736\n[Epoch  3 /  5]   * Counterclaim: 0.534722\n[Epoch  3 /  5]   * Rebuttal  : 0.434783\n[Epoch  3 /  5]   * Concluding Statement: 0.785690\n[Epoch  3 /  5]   * Position  : 0.685390\n[Epoch  3 /  5]\n[Epoch  3 /  5] Overall Validation F1: 0.6349 [147.09 secs]\n[Epoch  3 /  5]\n\n[Epoch  4 /  5] Starting epoch  4 with LR = 2.5e-06\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  4 /  5]     Steps:    0 --> Loss: 0.4685\n[Epoch  4 /  5]     Steps:  200 --> Loss: 0.4425\n[Epoch  4 /  5]     Steps:  400 --> Loss: 0.4268\n[Epoch  4 /  5]     Steps:  600 --> Loss: 0.4223\n[Epoch  4 /  5]     Steps:  800 --> Loss: 0.4240\n[Epoch  4 /  5]     Steps: 1000 --> Loss: 0.4218\n[Epoch  4 /  5]     Steps: 1200 --> Loss: 0.4228\n[Epoch  4 /  5]     Steps: 1400 --> Loss: 0.4270\n[Epoch  4 /  5]     Steps: 1600 --> Loss: 0.4234\n[Epoch  4 /  5]     Steps: 1800 --> Loss: 0.4219\n[Epoch  4 /  5]     Steps: 2000 --> Loss: 0.4204\n[Epoch  4 /  5]     Steps: 2200 --> Loss: 0.4196\n[Epoch  4 /  5]     Steps: 2400 --> Loss: 0.4198\n[Epoch  4 /  5]     Steps: 2600 --> Loss: 0.4213\n[Epoch  4 /  5]     Steps: 2800 --> Loss: 0.4216\n[Epoch  4 /  5]     Steps: 3000 --> Loss: 0.4219\n[Epoch  4 /  5]     Steps: 3200 --> Loss: 0.4230\n[Epoch  4 /  5]     Steps: 3400 --> Loss: 0.4227\n[Epoch  4 /  5]     Steps: 3600 --> Loss: 0.4226\n[Epoch  4 /  5]     Steps: 3800 --> Loss: 0.4224\n[Epoch  4 /  5]     Steps: 4000 --> Loss: 0.4229\n[Epoch  4 /  5]     Steps: 4200 --> Loss: 0.4230\n[Epoch  4 /  5]     Steps: 4400 --> Loss: 0.4230\n[Epoch  4 /  5]     Steps: 4600 --> Loss: 0.4228\n[Epoch  4 /  5]     Steps: 4800 --> Loss: 0.4230\n[Epoch  4 /  5]\n[Epoch  4 /  5] Training loss    : 0.4232\n[Epoch  4 /  5] Training accuracy: 0.8491\n[Epoch  4 /  5] Model saved to pytorch_model_e3.bin  [64.91 mins]\n[Epoch  4 /  5]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  4 /  5] Validation F1 scores\n[Epoch  4 /  5]   * Lead      : 0.801067\n[Epoch  4 /  5]   * Claim     : 0.542964\n[Epoch  4 /  5]   * Evidence  : 0.672311\n[Epoch  4 /  5]   * Counterclaim: 0.534653\n[Epoch  4 /  5]   * Rebuttal  : 0.435208\n[Epoch  4 /  5]   * Concluding Statement: 0.798500\n[Epoch  4 /  5]   * Position  : 0.689748\n[Epoch  4 /  5]\n[Epoch  4 /  5] Overall Validation F1: 0.6392 [147.11 secs]\n[Epoch  4 /  5]\n\n[Epoch  5 /  5] Starting epoch  5 with LR = 2.5e-07\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  5 /  5]     Steps:    0 --> Loss: 0.1559\n[Epoch  5 /  5]     Steps:  200 --> Loss: 0.3811\n[Epoch  5 /  5]     Steps:  400 --> Loss: 0.3841\n[Epoch  5 /  5]     Steps:  600 --> Loss: 0.3870\n[Epoch  5 /  5]     Steps:  800 --> Loss: 0.3903\n[Epoch  5 /  5]     Steps: 1000 --> Loss: 0.3870\n[Epoch  5 /  5]     Steps: 1200 --> Loss: 0.3881\n[Epoch  5 /  5]     Steps: 1400 --> Loss: 0.3893\n[Epoch  5 /  5]     Steps: 1600 --> Loss: 0.3883\n[Epoch  5 /  5]     Steps: 1800 --> Loss: 0.3898\n[Epoch  5 /  5]     Steps: 2000 --> Loss: 0.3896\n[Epoch  5 /  5]     Steps: 2200 --> Loss: 0.3883\n[Epoch  5 /  5]     Steps: 2400 --> Loss: 0.3867\n[Epoch  5 /  5]     Steps: 2600 --> Loss: 0.3860\n[Epoch  5 /  5]     Steps: 2800 --> Loss: 0.3880\n[Epoch  5 /  5]     Steps: 3000 --> Loss: 0.3876\n[Epoch  5 /  5]     Steps: 3200 --> Loss: 0.3880\n[Epoch  5 /  5]     Steps: 3400 --> Loss: 0.3877\n[Epoch  5 /  5]     Steps: 3600 --> Loss: 0.3873\n[Epoch  5 /  5]     Steps: 3800 --> Loss: 0.3873\n[Epoch  5 /  5]     Steps: 4000 --> Loss: 0.3868\n[Epoch  5 /  5]     Steps: 4200 --> Loss: 0.3866\n[Epoch  5 /  5]     Steps: 4400 --> Loss: 0.3861\n[Epoch  5 /  5]     Steps: 4600 --> Loss: 0.3857\n[Epoch  5 /  5]     Steps: 4800 --> Loss: 0.3856\n[Epoch  5 /  5]\n[Epoch  5 /  5] Training loss    : 0.3853\n[Epoch  5 /  5] Training accuracy: 0.8612\n[Epoch  5 /  5] Model saved to pytorch_model_e4.bin  [64.89 mins]\n[Epoch  5 /  5]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch  5 /  5] Validation F1 scores\n[Epoch  5 /  5]   * Lead      : 0.802555\n[Epoch  5 /  5]   * Claim     : 0.548313\n[Epoch  5 /  5]   * Evidence  : 0.674937\n[Epoch  5 /  5]   * Counterclaim: 0.535745\n[Epoch  5 /  5]   * Rebuttal  : 0.438679\n[Epoch  5 /  5]   * Concluding Statement: 0.795270\n[Epoch  5 /  5]   * Position  : 0.691815\n[Epoch  5 /  5]\n[Epoch  5 /  5] Overall Validation F1: 0.6410 [147.16 secs]\n[Epoch  5 /  5]\nFinal model saved as 'pytorch_model.bin'\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the data"
      ],
      "metadata": {
        "id": "ny2b0HlPowod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_df_test():\n",
        "    test_names, df_test = [], []\n",
        "    for f in list(os.listdir('../input/project/test')):\n",
        "        test_names.append(f.replace('.txt', ''))\n",
        "        df_test.append(open('../input/project/test/' + f, 'r').read())\n",
        "    df_test = pd.DataFrame({'id': test_names, 'text': df_test})\n",
        "    df_test['text_split'] = df_test.text.str.split()\n",
        "    return df_test\n",
        "\n",
        "df_test = load_df_test()\n",
        "df_test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:03:30.848875Z",
          "iopub.execute_input": "2023-12-04T23:03:30.849228Z",
          "iopub.status.idle": "2023-12-04T23:03:30.906272Z",
          "shell.execute_reply.started": "2023-12-04T23:03:30.849197Z",
          "shell.execute_reply": "2023-12-04T23:03:30.905280Z"
        },
        "trusted": true,
        "id": "k9omftFwowod",
        "outputId": "74a7baf5-c74e-47ec-8d03-a8a09f739457"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "             id                                               text  \\\n0  0FB0700DAF44  During a group project, have you ever asked a ...   \n1  D72CB1C11673  Making choices in life can be very difficult. ...   \n2  18409261F5C2  80% of Americans believe seeking multiple opin...   \n3  DF920E0A7337  Have you ever asked more than one person for h...   \n4  D46BCB48440A  When people ask for advice,they sometimes talk...   \n\n                                          text_split  \n0  [During, a, group, project,, have, you, ever, ...  \n1  [Making, choices, in, life, can, be, very, dif...  \n2  [80%, of, Americans, believe, seeking, multipl...  \n3  [Have, you, ever, asked, more, than, one, pers...  \n4  [When, people, ask, for, advice,they, sometime...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0FB0700DAF44</td>\n      <td>During a group project, have you ever asked a ...</td>\n      <td>[During, a, group, project,, have, you, ever, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D72CB1C11673</td>\n      <td>Making choices in life can be very difficult. ...</td>\n      <td>[Making, choices, in, life, can, be, very, dif...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18409261F5C2</td>\n      <td>80% of Americans believe seeking multiple opin...</td>\n      <td>[80%, of, Americans, believe, seeking, multipl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DF920E0A7337</td>\n      <td>Have you ever asked more than one person for h...</td>\n      <td>[Have, you, ever, asked, more, than, one, pers...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D46BCB48440A</td>\n      <td>When people ask for advice,they sometimes talk...</td>\n      <td>[When, people, ask, for, advice,they, sometime...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = tokenize(df_test, with_labels=False) #Using the same tokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:06:16.349398Z",
          "iopub.execute_input": "2023-12-04T23:06:16.350230Z",
          "iopub.status.idle": "2023-12-04T23:06:16.381019Z",
          "shell.execute_reply.started": "2023-12-04T23:06:16.350197Z",
          "shell.execute_reply": "2023-12-04T23:06:16.379779Z"
        },
        "trusted": true,
        "id": "17LkUlOGowod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_test['input_ids'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:06:34.269630Z",
          "iopub.execute_input": "2023-12-04T23:06:34.270803Z",
          "iopub.status.idle": "2023-12-04T23:06:34.278443Z",
          "shell.execute_reply.started": "2023-12-04T23:06:34.270756Z",
          "shell.execute_reply": "2023-12-04T23:06:34.277219Z"
        },
        "trusted": true,
        "id": "c-pnDeZtowoe",
        "outputId": "5e42347a-d670-4e87-c2bd-ddbe49d11626"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "10"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:06:40.954545Z",
          "iopub.execute_input": "2023-12-04T23:06:40.954881Z",
          "iopub.status.idle": "2023-12-04T23:06:40.963115Z",
          "shell.execute_reply.started": "2023-12-04T23:06:40.954854Z",
          "shell.execute_reply": "2023-12-04T23:06:40.962009Z"
        },
        "trusted": true,
        "id": "xVNvD0-aowoe",
        "outputId": "65c1994d-cd26-46d6-8c12-6b76e5d06b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['text_split'].str.len()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:06:59.109666Z",
          "iopub.execute_input": "2023-12-04T23:06:59.110107Z",
          "iopub.status.idle": "2023-12-04T23:06:59.120567Z",
          "shell.execute_reply.started": "2023-12-04T23:06:59.110043Z",
          "shell.execute_reply": "2023-12-04T23:06:59.119313Z"
        },
        "trusted": true,
        "id": "wNJ_K-9fowoe",
        "outputId": "41570ad7-9418-488e-90ba-ef91eb4cf1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0     635\n1     421\n2    1056\n3     711\n4     363\nName: text_split, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = len(tokenizer(df_test.iloc[2]['text'])['input_ids'])\n",
        "n_tokens"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:07:16.962138Z",
          "iopub.execute_input": "2023-12-04T23:07:16.962894Z",
          "iopub.status.idle": "2023-12-04T23:07:16.981390Z",
          "shell.execute_reply.started": "2023-12-04T23:07:16.962857Z",
          "shell.execute_reply": "2023-12-04T23:07:16.980285Z"
        },
        "trusted": true,
        "id": "rOGlTRygowof",
        "outputId": "3fc114fa-2fbb-4794-cf07-e1d36b86f540"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
          "output_type": "stream"
        },
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "1304"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verification that 4 chunks of 512 with a stride of 200 is the correct number of chunks to fit 1304 tokens in\n",
        "# 512 + 2*(512-200) < n_tokens < 512 + 3*(512-200)"
      ],
      "metadata": {
        "id": "uPgbyojxowof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Original text:\n",
        "df_test.iloc[2]['text']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:08:00.303052Z",
          "iopub.execute_input": "2023-12-04T23:08:00.303874Z",
          "iopub.status.idle": "2023-12-04T23:08:00.312078Z",
          "shell.execute_reply.started": "2023-12-04T23:08:00.303840Z",
          "shell.execute_reply": "2023-12-04T23:08:00.310893Z"
        },
        "trusted": true,
        "id": "XIRntmKxowof",
        "outputId": "7b81334d-9041-48ad-c9b9-56fadb614b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"80% of Americans believe seeking multiple opinions can help them make better choices, and for good reason. Studies have shown the average Americans faring far better in their lives compared to their counterparts because they are listening to other's advice. There are also many myths that have the moral of listening to other people's opinions. For example, Perseus got his achievement of slaying a gorgon because he listened to the Oracle. Another example I have is the fable of Osiris, in which Osiris listens to Sekhmet and becomes the king of he underworld. In all of these stories, the hero listens to other people, and benefited from the people around them being more knowledgeable, more experienced, and giving the hero more choices to consider. Therefore, I believe listening to other's advice can help someone make a better choice because they have more experience compared to you, know the pros and cons of your choice, and gives you multiple perspectives to deliberate over.\\n\\nFor example, let us talk about the anecdote of little Generic_Name, whose life was changed forever by listening to a person who has more experience than him. As a young lad, little Generic_Name always dreamed of becoming a soldier, because he admired his mother who fought in WW2. But Generic_Name's mother, who lived through the inhospitable battlefield, didnt want her child to follow her path. As a result, she talked to him about the hardships that plagued her when she was a soldier, and successfully changed his mind. Thanks to Generic_Name's mother changing little Generic_Name's mind, He became a successful and joyous doctor who saves countless of lives every day. If Generic_Name's mother hadn't been a soldier who has lived through the darkest depths of hell called the battlefield, then she wouldn't have been able to change little Generic_Name's mind, and he may have had his life cut short. Another story we can talk about is the story of little Generic_Name, who always wanted to be a magician. Generic_Name always fantasizes about being on stage, listening to the crowds wild applause as she entertains them, and thankfully, her father is there to fulfill her fantasies. Thanks to Generic_Name's father being a renowned magician, he had the connections and knew the tricks that could excite the crowd. As a result, Generic_Name became the greatest magician of that generation. If Generic_Name's father wasnt a magician who had walked the path Generic_Name was planning to walk, then she wouldnt have been able to reach the heights she was reaching right now. Therefore, I believe that listening to people who have more experiences compared to you an help you make a better judgment.\\n\\nAnother anecdote we can talk about is the anecdote of Generic_Name, a menacing ancient man who knew the pros and cons of doing drugs. As Generic_Name was walking the streets of New Jersey, he saw a group about to eat drugs. Generic_Name walked towards the group, and he latched his hand on to the nearest person's, and stopped the person from doing some drugs. Generic_Name reprimanded the group about doing drugs, and thanks to his menacing presence, the group stopped downing some pills. Thanks to the effort of Generic_Name who lectured the group about the pros and cons of eating drugs, the group was saved from the evil influences of drugs, and grew up to be a group who fought against drug use. If Generic_Name wasn't telling the group about the pros and cons of drugs, they would have eaten it and fallen into a path which is very hard to recover from. But thankfully, Generic_Name saved them from that despairing future. Another anecdote I want to talk about is the story of Generic_Name, who wanted o be doctor. Generic_Name didn't know much about being a doctor, except that she wanted to be one, so her grandfather talked with her about her choice. As they discussed the pros and cons of being a doctor, Generic_Name grew more knowledgeable about the choice she is going to make. As a result, Generic_Name is much more firm in her beliefs about being a doctor, and grew up to be an amazing doctor. If she didnt' talk with her grandfather about her choice, she would have made an uniformed decision that likely would have ruined her life. Therefore, I believe that listening to other's advice can help someone make a better choice by giving them the pros and cons of their choice.\\n\\nOne anecdote I want to talk about is the story of Generic_Name, who wanted to be a lawyer because her friend's advised her. She was steadfast in her beliefs as she always enjoyed rules and regulations, until her mother gave her a new perspective to mull over. She told her to aim to become a supreme dragon, rather than being a struggling python.\\n\\nAs a result, Generic_Name aimed to be a supreme judge, and she became one at the age of 65. Thanks to her mother giving her better advice, She became a star that outshone other's in the world of law. Another anecdote I want to talk about is the story of Generic_Name whose life changed thanks to a fateful encounter that changed his life. Generic_Name felt forced to be a fashion designer, as it was a family tradition.\\n\\nHe didn't know what to do, until he heard his grandmother talking with his family about animal cloning. He grew interested in animal cloning and researched it until he considered it a his future career. As a result, he grew up to become a biologist, doing something he loves. If it wasn't for his grandmother telling Generic_Name's family members about animal cloning, he wouldn't have even considered it, and may have grown to be a nihilistic person who isn't able to change his fate of suffering. Therefore, I believe seeking multiple opinions can help someone make a better choice because it can give them multiple perspectives to consider.\\n\\nIn conclusion, little Generic_Name, Generic_Name, Generic_Name, and many more changed their entire lives by listening to other people. If they didn't seek other's advice, they would have fallen into a path that would not allow their potential to bloom. But, thankfully, they chose to listen to other people. As a result, they grew to be figures that were respected around the world. Therefore, I in believe seeking multiple opinions.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The four 512-token chunks generated by the tokenization procedure:\n",
        "tokenizer.decode(tokenized_test['input_ids'][3])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:08:09.798570Z",
          "iopub.execute_input": "2023-12-04T23:08:09.798936Z",
          "iopub.status.idle": "2023-12-04T23:08:09.808344Z",
          "shell.execute_reply.started": "2023-12-04T23:08:09.798905Z",
          "shell.execute_reply": "2023-12-04T23:08:09.807270Z"
        },
        "trusted": true,
        "id": "r-dNatbXowog",
        "outputId": "fb6c0bc0-2803-4381-d73b-2405f26f70ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 41,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"<s> 80% of Americans believe seeking multiple opinions can help them make better choices, and for good reason. Studies have shown the average Americans faring far better in their lives compared to their counterparts because they are listening to other's advice. There are also many myths that have the moral of listening to other people's opinions. For example, Perseus got his achievement of slaying a gorgon because he listened to the Oracle. Another example I have is the fable of Osiris, in which Osiris listens to Sekhmet and becomes the king of he underworld. In all of these stories, the hero listens to other people, and benefited from the people around them being more knowledgeable, more experienced, and giving the hero more choices to consider. Therefore, I believe listening to other's advice can help someone make a better choice because they have more experience compared to you, know the pros and cons of your choice, and gives you multiple perspectives to deliberate over. For example, let us talk about the anecdote of little Generic_Name, whose life was changed forever by listening to a person who has more experience than him. As a young lad, little Generic_Name always dreamed of becoming a soldier, because he admired his mother who fought in WW2. But Generic_Name's mother, who lived through the inhospitable battlefield, didnt want her child to follow her path. As a result, she talked to him about the hardships that plagued her when she was a soldier, and successfully changed his mind. Thanks to Generic_Name's mother changing little Generic_Name's mind, He became a successful and joyous doctor who saves countless of lives every day. If Generic_Name's mother hadn't been a soldier who has lived through the darkest depths of hell called the battlefield, then she wouldn't have been able to change little Generic_Name's mind, and he may have had his life cut short. Another story we can talk about is the story of little Generic_Name, who always wanted to be a magician. Generic_Name always fantasizes about being on stage, listening to the crowds wild applause as she entertains them, and thankfully, her father is there to fulfill her fantasies. Thanks to Generic_Name's father being a renowned magician, he had the connections and knew the tricks that could excite the crowd. As a result, Generic_Name became the greatest magician of that generation. If Generic_Name's father wasnt a magician who had walked the path Generic_Name was planning to walk, then she wouldnt have been</s>\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_test['input_ids'][6])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T23:08:27.570181Z",
          "iopub.execute_input": "2023-12-04T23:08:27.570561Z",
          "iopub.status.idle": "2023-12-04T23:08:27.579057Z",
          "shell.execute_reply.started": "2023-12-04T23:08:27.570528Z",
          "shell.execute_reply": "2023-12-04T23:08:27.577972Z"
        },
        "trusted": true,
        "id": "uNYVJu9bowog",
        "outputId": "e1124e59-1866-455f-e62b-bea1d3eb72f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"<s> he wouldn't have even considered it, and may have grown to be a nihilistic person who isn't able to change his fate of suffering. Therefore, I believe seeking multiple opinions can help someone make a better choice because it can give them multiple perspectives to consider. In conclusion, little Generic_Name, Generic_Name, Generic_Name, and many more changed their entire lives by listening to other people. If they didn't seek other's advice, they would have fallen into a path that would not allow their potential to bloom. But, thankfully, they chose to listen to other people. As a result, they grew to be figures that were respected around the world. Therefore, I in believe seeking multiple opinions.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
          },
          "metadata": {}
        }
      ]
    }
  ]
}